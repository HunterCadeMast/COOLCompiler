# Hunter Mast / hunter.c.mast@vanderbilt.edu
# This is where I recieved most information on the PLY lexer: https://ply.readthedocs.io/en/latest/

	When creating my lexer, there was a lot of different conditions I had to consider. I tried to replicate the COOL built-in lexer as best as I could. I started off by creating an array with all of my tokens inside. This list was given to us by Dr. Leach, so it made it pretty easy to implement. For using PLY, the way it works is that it reads different variables or definitions in the format of "t_TOKEN" for all of my tokens. For most, I just made a definition with regular expression of the symbol that is being read from the COOL code. For example, if ":" is found, we recognized that our token is "COLON" due to the regular expression being r":". I am already very familiar with regular expression, so this section wasn't too hard to do. I did have to remember to use "\" to get certain characters into the strings though like "*", for example.

	For CLASS, COLON, INHERITS, and NEW, I had to create a condition to them. When I was initially creating the lexer, I kept having issues with some tokens being recognized as an IDENTIFIER, not a TYPE. To combat this, anything after the few tokens I listed above is always a TYPE. So adding "type_check", I would always correctly be able to label something as a TYPE and not an IDENTIFIER. Other token types reset "type_check" to false if it does not happen to be a type after one of these few tokens. "type_check" is included as a part of my lexer in PLY since I called lexer.type_check, so I can now change this as if it is an element of a token. So t.lexer.type_check in token definitions will change it from true to false and vice versa. The same is done with lexer.error_occurred. This is so I don't need to define any global variables and save some space. After a while though, I realized that TYPE and IDENTIFIER are different inherently.TYPE must start with a capital letter while IDENTIFIER must be lowercase. I copied my code for IDENTIFIER and just changed it to allow for capital letter instead of lowercase.

	Another issue I ran into when creating my lexer is the order that my tokens were in. The biggest culprit of this issue was with INHERITS and IN. Since it reads top to bottom, line by line, IN used to be above INHERITS and when INHERITS was being read, it would seperate "IN" and "herits". It would find one type really fast and just stop. The easy way I went about fixing this was just to move INHERITS to be checked first so that if there is a whitespace after "IN", then it is not INHERITS and is IN. LET and LE also seemed prone to this issue, so I switched it to where LET comes first, but I don't think this woul necessarily be true. LE is not checking for a string similar to LET, as LET checks for "let" and LE checks for "<=". I did switch them though just to be safe as it does not change anything drastic by doing this.

	Where we start getting into more complex code is when we go through t_newline, t_error, t_IDENTIFIER, t_INTEGER, and t_STRING. For t_IDENTIFIER and t_TYPE, I ran a regular expression string that would read the first letter of each identifier or type and make sure it is a letter. If that works, then anything after it for the rest of the string can be any letter, number, or underscore. Something else that was added to both of these are dealing with a lone underscore and object ID. For both of these, I just added a quick check if objectID is sound and to send an error or if an underscore is found alone. If something starts with an underscore, then it should just send an error also.

	For t_INTEGER, we just check if any digits exist by themself. If so, change the string to an int and send that as the value. This really only has one error check and that is if t.value is above the maximum number we can store in a 32-bit integer (2147483647), we throw an error. Something very similar to t_INTEGER I added is t_HEX_OCTAL. This here is similar to checking for a TYPE or IDENTIFIER, but a bit different. We try and find if something starts with a 0, the next letters should be something like a 'x' or an 'o', to signify hex or octal. COOL happens to seperate these into 0x as one character and the rest of the digits as a string, so I replicated that here. 

	t_STRING is a bit more complex than the others. For it, we need to get anything between quotations. With this, it can be between lines and also contain \n as a string. We ahve to accept all of this as an option. With it, I also made it non-greedy. If we were to have "ONE" + "TWO" as a string, it would most likely include the plus for the string ONE" + "TWO if we just used *. Using *?, we get ONE and TWO seperately. The COOL lexer prints out in reference.cl that any string does not have quotations included. I tried to incorporate this also, so I had to cut off the 1st character and 2nd character so that quotations are not printed out for each string. I had to include a few more checks for this also. Built in, t_STRING should already be checking for matching quotation marks. If we only find one, then it throws an error by t_error. If it somehow gets past that though, we check to see if the last string is not a quotation. If not, we throw an error. We cut off the quotations at the end after this part. We also make sure there is no NULL value inside of the string. This is done by looking for '\x00', which signifies NULL. Finally, we check that the string length does not exceed the maximum we can store, which is 1024 characters I believe.

	t_newline checks for any time we are at the end of the line and anytime there is a line that is just blank. If we do run into that, we up our line number by however many lines we skip. If there are 3 blank lines, our line number goes up by 3. This is for error tracking mostly. 

	Next, we have t_error. This works with new_line and if anything is not one of our tokens, we write what line the error is on and what the chracter is. This was a little more complex for me as I was trying to as close to the actual lexer as possible. The actual lexer for COOL actually stops after encountering any error and deletes all of the contents it has already written down. I had to add a check to make sure that it knew an error was recorded and to both stop reading the file and delete all the contents recorded already.
	
	After all of the token definitions are called, we finally get to the main part of the funciton. Here we initialize the lexer, get the file name from command line, and read all of the contents of the file. We throw an error if we cannot find any file name and stop the program. After that, we read the data, create a new file, and then start to write to the new file. We just write down the line number, lowercase type, and then the actual token. We do lowercase to match how the COOL lexer does it. All of this is pretty simple and doesn't have anything too special.

	The most difficult part of all this for me was dealing with comments. I did not realize that states were a thing until after I finished this, so I did all of this checknig manually. For a regular comment, I just checked for if there is "--". If so, just ignore the rest of the line. Pretty simple. Multi-line comments were the biggest pain though. These comments could be nested. If we had (*(* RANDOM COMMENT *)*), I would have to make sure we don't stop at that first instance of *). To do this, I created a definition that would could how many times we find (* and continue to go 'deeper' into how far we are into the multi-line comment. Once we found our first instance of a multi-line comment, we would go character by character until we found either (* and added one to our depth or *) and then took one away. Once we reached a depth less than zero, we would exit the comment. If we reached the end of the file without finding the end, we would throw an error. This was difficult for a few reasons. One was making sure to correctly count lines. I had to be constantly checking the lexer data to get each character and once we reached the end of the file, that last check out throw an error as it would out of the index. Add this check would cause so many errors because it would also cause errors for line counting if we ended right before the end. It took a while, but I finally got a version of it that would end if we are at the very end and throw an EOF error. I tried it a bit differently before this version and it somehow messed up and conflicted with my strings, so I am glad that this version works with that.

	Something else that I tried to deal with, but was not able to figure out was comments. When doing comments, COOL has it as two dashes. I wanted to check if the dashes are at the beginning of a line and then skip to the next line, but for some reason, it would always read the whole line. I decided to ignore it for now, but I will come back to it if that is something that needs lexing. The types given do not specify it, so I want to not focus on it right now.

	For good.cl, I designed it to try and get as many types checked as possible. The token types I checked were: CLASS, TYPE, LBRACE, IDENTIFIER, COLON, LARROW, SMI, INTEGER, STRING LPAREN, RPAREN, MINUS, DIVIDE, COMMA, FALSE, TIMES, WHILE, LOOP, POOL, LET, IN, LE, NOT, THEN, PLUS, RBRACE, IF, FI, ELSE, and EQUALS. The main few I never checked with the good case was ISVOID, DOT, ESAC, TILDE, and AT. These are more specific and I didn't really see any good place for them in here. good.cl also is a basic price checker that does lots of out_strings and out_int. One check it does is capital letters and showing that if we have 'lEt', it still works as 'let'.  There is also a nested comment with both a broken string inside and a single line comment in the code. All together, this is a working program that checks practically everything that could possibly come up when lexing.

	For bad.cl, it's a lot more simple. With it, we have some syntax that is not fully correct. Here, we are only checknig on lexing and if all the tokens are matched correctly, so this makes sure we are not accidently doing anything that we shouldn't be doing yet. I also included some of the missing tokens we did not use in good.cl. They are not implemented, but they are read to make sure they are correctly being seen as tokens. Two main characters that should not be read are "%" and "/". The lexer should just stop and delete all of it's progress when it runs into an error, so it should only run into "%" and not compute or send an error out for "/". There is also an error with the line after "/", being 123abc as variables and integers should not have any numbers with it. The 00123 should work, but we will never get that far in the program to check it. I deleted the last 2 lines of my lexer to just show and test that DOT, ISVOID, ESAC, AT, and TILDE do work. The error I dexided to mainly have to this was a nested comment not working, but if you remove that it should have an error with '%'. Lone underscore and a variable starting with one also are thrown if far enough. A lot of my testing was adding these parts and removing them as it would throw errors and stop the program. Something I wish I did with this was create a bunch of different files with many small tests for them. Next assignment I plan on doing that as it would be great for the final assignment to run through all of the small COOL files to check for specific things.

	Another large help with testing and checking was arith.cl. I remembered I had a copy from PA1, so I used that as it is also a test on PA2. This was great for helping me test for errors with integers and symbols. It was a large scale COOL program that pretty much had everything in it, so I was able to try a variety of things to get my lexer working properly.

bad.cl Output without Final Lines in Main.py:
11
dot
13
isvoid
15
esac
17
at
19
tilde

	Here, we show that it will print them all out as the token they are. Since we are not checking syntax, it does not matter that these are not correctly implemented. I also had to delete the last to lines because I delete the whole file if an error is found.

	Overall, I designed the program as closely to the actual COOL lexer as I could. The biggest difference is that I did not include the lexing of comments. It should lex everything the exact same way. good.cl and bad.cl also check every token along with showing how errors will be reported. My lexer is very detailed and to me, feels very optimized. There are some changes I could possibly make like making some token definitions just the regular expression code and not an entire definition, but I think it looks cleaner right now without having some as definitions and some not. Something else that I've struggled on and did not figure out how to finish was dealing with whitespace. I thought this would be fixed from all my other errors or checks, but nothing fixed it. I added \v to my ignore statement, but it still has an error for it. I may try to optimize and fix it if I cannot before the due date of this assignment. It still works great overall and I am very proud of what I've been able to accomplish with this project so far.